{"version":3,"sources":["tokenizer.test.js"],"names":["tokenizer","require","it","tokens","tokenize","expect","toEqual","input"],"mappings":";;AAAA,IAAIA,SAAS,GAAGC,OAAO,CAAC,kBAAD,CAAvB;;AAEAC,EAAE,CAAC,qBAAD,EAAwB,YAAY;AAClC,MAAIC,MAAM,GAAGH,SAAS,CAACI,QAAV,CAAmB,KAAnB,CAAb;AACAC,EAAAA,MAAM,CAACF,MAAD,CAAN,CAAeG,OAAf,CAAuB,CAAC,KAAD,CAAvB;AACH,CAHC,CAAF;AAKAJ,EAAE,CAAC,wBAAD,EAA2B,YAAY;AACvC,MAAIK,KAAK,GAAG,oBAAZ;AACA,MAAIJ,MAAM,GAAGH,SAAS,CAACI,QAAV,CAAmBG,KAAnB,CAAb;AACAF,EAAAA,MAAM,CAACF,MAAD,CAAN,CAAeG,OAAf,CAAuB,CAAC,KAAD,EAAQ,KAAR,EAAe,MAAf,EAAuB,GAAvB,EAA4B,GAA5B,EAAiC,GAAjC,CAAvB;AACD,CAJC,CAAF;AAMAJ,EAAE,CAAC,+BAAD,EAAkC,YAAY;AAC9C,MAAIK,KAAK,GAAG,mBAAZ;AACA,MAAIJ,MAAM,GAAGH,SAAS,CAACI,QAAV,CAAmBG,KAAnB,CAAb;AACAF,EAAAA,MAAM,CAACF,MAAD,CAAN,CAAeG,OAAf,CAAuB,CAAC,KAAD,EAAQ,KAAR,EAAe,MAAf,EAAuB,MAAvB,CAAvB;AACD,CAJC,CAAF;AAMAJ,EAAE,CAAC,cAAD,EAAiB,YAAY;AAC7B,MAAIK,KAAK,GAAG,wBAAZ;AACA,MAAIJ,MAAM,GAAGH,SAAS,CAACI,QAAV,CAAmBG,KAAnB,CAAb;AACAF,EAAAA,MAAM,CAACF,MAAD,CAAN,CAAeG,OAAf,CAAuB,CAAC,MAAD,EAAS,GAAT,EAAc,IAAd,EAAoB,GAApB,EAAyB,MAAzB,EAAiC,KAAjC,EAAwC,GAAxC,CAAvB;AACD,CAJC,CAAF;AAMAJ,EAAE,CAAC,gBAAD,EAAmB,YAAY;AAC/B,MAAIK,KAAK,GAAG,qBAAZ;AACA,MAAIJ,MAAM,GAAGH,SAAS,CAACI,QAAV,CAAmBG,KAAnB,CAAb;AACAF,EAAAA,MAAM,CAACF,MAAD,CAAN,CAAeG,OAAf,CAAuB,CAAC,MAAD,EAAS,GAAT,EAAc,IAAd,EAAoB,KAApB,EAA2B,IAA3B,EAAiC,IAAjC,CAAvB;AACD,CAJC,CAAF;AAMAJ,EAAE,CAAC,gDAAD,EAAmD,YAAa;AAChE,MAAIK,KAAK,GAAG,gBAAZ;AACA,MAAIJ,MAAM,GAAGH,SAAS,CAACI,QAAV,CAAmBG,KAAnB,CAAb;AACAF,EAAAA,MAAM,CAACF,MAAD,CAAN,CAAeG,OAAf,CAAuB,CAAC,gBAAD,CAAvB;AACD,CAJC,CAAF;AAMAJ,EAAE,CAAC,gCAAD,EAAmC,YAAU;AAC7C,MAAIK,KAAK,GAAG,oBAAZ;AACA,MAAIJ,MAAM,GAAGH,SAAS,CAACI,QAAV,CAAmBG,KAAnB,CAAb;AACAF,EAAAA,MAAM,CAACF,MAAD,CAAN,CAAeG,OAAf,CAAuB,CAAC,GAAD,EAAM,OAAN,EAAe,UAAf,EAA2B,GAA3B,CAAvB;AACD,CAJC,CAAF;AAMAJ,EAAE,CAAC,4BAAD,EAA+B,YAAW;AAC1C,MAAIK,KAAK,GAAG,WAAZ;AACA,MAAIJ,MAAM,GAAGH,SAAS,CAACI,QAAV,CAAmBG,KAAnB,CAAb;AACAF,EAAAA,MAAM,CAACF,MAAD,CAAN,CAAeG,OAAf,CAAuB,CAAC,KAAD,EAAQ,IAAR,EAAc,IAAd,CAAvB;AACD,CAJC,CAAF","sourcesContent":["var tokenizer = require('../lib/tokenizer');\r\n\r\nit(\"handles single word\", function () {\r\n    var tokens = tokenizer.tokenize('foo');\r\n    expect(tokens).toEqual(['foo']);\r\n});\r\n\r\nit(\"handles multiple words\", function () {\r\n  var input = \"plz foo with a b c\";\r\n  var tokens = tokenizer.tokenize(input);\r\n  expect(tokens).toEqual(['plz', 'foo', 'with', 'a', 'b', 'c']);\r\n});\r\n\r\nit(\"handles escaped single quotes\", function () {\r\n  var input = \"plz foo with '\\''\";\r\n  var tokens = tokenizer.tokenize(input);\r\n  expect(tokens).toEqual([\"plz\", \"foo\", \"with\", \"'\\''\"]);\r\n});\r\n\r\nit(\"handles json\", function () {\r\n  var input = \"very x is { foo: bar }\";\r\n  var tokens = tokenizer.tokenize(input);\r\n  expect(tokens).toEqual([\"very\", \"x\", \"is\", \"{\", \"foo:\", \"bar\", \"}\"]);\r\n});\r\n\r\nit(\"handles arrays\", function () {\r\n  var input = \"very x is [1, 2, 3]\";\r\n  var tokens = tokenizer.tokenize(input);\r\n  expect(tokens).toEqual([\"very\", \"x\", \"is\", \"[1,\", \"2,\", \"3]\"]);\r\n});\r\n\r\nit(\"handles string with spaces without breaking it\", function ()  {\r\n  var input = \"'hello world!'\";\r\n  var tokens = tokenizer.tokenize(input);\r\n  expect(tokens).toEqual([\"'hello world!'\"]);\r\n});\r\n\r\nit(\"handles json with string value\", function(){\r\n  var input = \"{ some: \\\"json\\\" }\"\r\n  var tokens = tokenizer.tokenize(input);\r\n  expect(tokens).toEqual([\"{\", \"some:\", \"\\\"json\\\"\", \"}\"]);\r\n});\r\n\r\nit(\"handles array declarations\", function() {\r\n  var input = \"[1, 2, 3]\"\r\n  var tokens = tokenizer.tokenize(input);\r\n  expect(tokens).toEqual([\"[1,\", \"2,\", \"3]\"]);\r\n});\r\n"]}